{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cundeyu154/PL-Repo/blob/main/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-genai pandas numpy requests beautifulsoup4 scikit-learn jieba gradio gspread"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WSNNAhLUJrp",
        "outputId": "8703af17-54a9-4a7e-e9c0-f453fe7960cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.45.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba\n",
        "import gradio as gr\n",
        "import gspread\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. 配置與常量 (請替換為您的實際值！)\n",
        "# ==============================================================================\n",
        "GEMINI_API_KEY = \"AIzaSyDN-LWFIop1hMP2J3lKLN8hU06ZOb3tkvM\"\n",
        "\n",
        "GS_CREDENTIALS_PATH = \"service_account_key.json\"\n",
        "\n",
        "# Google Sheet 設置\n",
        "SHEET_NAME = \"文本分析自動化專案\"\n",
        "DATA_WORKSHEET_TITLE = \"原始數據\" # 爬蟲結果存放的工作表名稱\n",
        "STATS_WORKSHEET_TITLE = \"統計結果\" # 關鍵詞統計結果存放的工作表名稱\n",
        "TOP_N_KEYWORDS = 20 # 要提取的熱詞數量\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. 輔助函數：Google Sheets 處理\n",
        "# ==============================================================================\n",
        "\n",
        "def init_gspread():\n",
        "    \"\"\"初始化 gspread 客戶端並授權.\"\"\"\n",
        "    try:\n",
        "        # 使用服務帳戶金鑰檔案進行授權\n",
        "        gc = gspread.service_account(filename=GS_CREDENTIALS_PATH)\n",
        "        return gc\n",
        "    except Exception as e:\n",
        "        # 在 Gradio 介面中拋出錯誤，讓使用者看到\n",
        "        raise gr.Error(f\"Google Sheets 授權失敗。請檢查設定。錯誤: {e}\")\n",
        "\n",
        "def get_or_create_worksheet(spreadsheet, title, header):\n",
        "    \"\"\"取得或創建指定名稱的工作表。\"\"\"\n",
        "    try:\n",
        "        worksheet = spreadsheet.worksheet(title)\n",
        "        return worksheet\n",
        "    except gspread.WorksheetNotFound:\n",
        "        # 如果工作表不存在，則創建它並寫入標題\n",
        "        worksheet = spreadsheet.add_worksheet(title=title, rows=\"100\", cols=\"20\")\n",
        "        worksheet.update('A1', [header])\n",
        "        return worksheet\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"操作工作表 {title} 失敗: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. 數據收集：爬蟲功能 (已強化為實際提取)\n",
        "# ==============================================================================\n",
        "\n",
        "def scrape_data(target_url):\n",
        "    \"\"\"\n",
        "    從單一 URL 爬取文章主體文本的強化版函數。\n",
        "    它嘗試使用常見的 CSS 選擇器來定位主要文章內容。\n",
        "    \"\"\"\n",
        "    url = target_url\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        url = 'https://' + url\n",
        "\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status() # 檢查 HTTP 錯誤\n",
        "\n",
        "        # 這裡設置編碼處理，避免中文亂碼\n",
        "        response.encoding = response.apparent_encoding\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # 強化：嘗試使用更精確的選擇器來定位文章主體\n",
        "        content_selectors = [\n",
        "            \"article\",\n",
        "            \".article-content\",\n",
        "            \".post-body\",\n",
        "            \".entry-content\",\n",
        "            \".main-content\",\n",
        "            \"#content-body\", # 針對特定網站結構添加\n",
        "        ]\n",
        "\n",
        "        main_content_element = None\n",
        "        for selector in content_selectors:\n",
        "            # 嘗試使用 CSS 選擇器查找\n",
        "            main_content_element = soup.select_one(selector)\n",
        "            if main_content_element:\n",
        "                break\n",
        "\n",
        "        paragraphs = []\n",
        "        if main_content_element:\n",
        "            # 如果找到主體元素，只從該元素內提取所有段落\n",
        "            paragraphs = [p.get_text().strip() for p in main_content_element.find_all('p')]\n",
        "        else:\n",
        "            # 如果沒有找到精確的主體元素，則退回到全頁面的 <p> 標籤\n",
        "            paragraphs = [p.get_text().strip() for p in soup.find_all('p')]\n",
        "\n",
        "        # 過濾空行並合併\n",
        "        full_text = '\\n'.join(filter(None, paragraphs))\n",
        "\n",
        "        # 如果無法提取大量文本，可能是網站結構複雜，或頁面非主要文章\n",
        "        if not full_text or len(full_text) < 100:\n",
        "            # 嘗試抓取 body 裡的所有文本作為最終備選\n",
        "            body_text = soup.body.get_text(separator='\\n', strip=True) if soup.body else \"\"\n",
        "            if len(body_text) > 100:\n",
        "                 full_text = body_text\n",
        "            else:\n",
        "                return pd.DataFrame({\"標題\": [\"無法提取主體文本\"], \"內容\": [\"請檢查 URL、網站結構或爬蟲邏輯。\"]})\n",
        "\n",
        "        # 模擬單筆數據的 DataFrame 結構\n",
        "        data = pd.DataFrame({\n",
        "            \"標題\": [soup.title.string.strip() if soup.title else \"無標題\"],\n",
        "            \"內容\": [full_text]\n",
        "        })\n",
        "        return data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        error_msg = f\"連線錯誤: {e.__class__.__name__} - {e}\"\n",
        "        return pd.DataFrame({\"標題\": [error_msg], \"內容\": [error_msg]})\n",
        "    except Exception as e:\n",
        "        error_msg = f\"爬蟲解析錯誤: {e.__class__.__name__} - {e}\"\n",
        "        return pd.DataFrame({\"標題\": [error_msg], \"內容\": [error_msg]})\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. 文本分析：TF-IDF 與關鍵詞提取\n",
        "# ==============================================================================\n",
        "\n",
        "def chinese_tokenizer(text):\n",
        "    \"\"\"使用 jieba 進行中文分詞。\"\"\"\n",
        "    # 擴充更完整的中文停用詞列表\n",
        "    # 這裡只使用一個非常簡化的停用詞列表，實際應用中請擴充\n",
        "    simple_stopwords = {'的', '了', '是', '我', '你', '他', '她', '它', '我們', '你們', '他們', '和', '但', '也', '而', '這', '那', '一個', '一種', '可以', '能夠', '因為', '所以', '這樣', '那樣', '在', '上', '下', '中', '之', '所', '則', '等', '與', '或'}\n",
        "\n",
        "    # 使用 jieba.cut 進行精確分詞\n",
        "    words = jieba.cut(text, cut_all=False)\n",
        "\n",
        "    # 過濾停用詞、單字詞（通常為助詞/標點）、數字和空白\n",
        "    filtered_words = [\n",
        "        word.lower()\n",
        "        for word in words\n",
        "        if word.strip() and\n",
        "           len(word.strip()) > 1 and\n",
        "           word.lower() not in simple_stopwords and\n",
        "           not word.strip().isdigit()\n",
        "    ]\n",
        "\n",
        "    # 必須返回空格分隔的字符串，這是 TfidfVectorizer 期望的格式\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def dummy_tokenizer(text):\n",
        "    \"\"\"TfidfVectorizer 需要一個接受文本並返回分詞後的空格分隔字串的 callable。\"\"\"\n",
        "    return chinese_tokenizer(text).split(' ')\n",
        "\n",
        "def perform_analysis(df):\n",
        "    \"\"\"執行 TF-IDF 計算，提取熱詞。\"\"\"\n",
        "    if df.empty or '內容' not in df.columns:\n",
        "        return None, \"數據框無內容或格式錯誤。\"\n",
        "\n",
        "    corpus = df['內容'].astype(str).tolist()\n",
        "\n",
        "    # 使用 TfidfVectorizer 進行 TF-IDF 計算\n",
        "    # 我們將 preprocessor 和 tokenizer 設為 None，讓 token_pattern 處理，\n",
        "    # 但為了中文精確分詞，我們必須用自己的分詞函數\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        tokenizer=dummy_tokenizer, # 使用 dummy_tokenizer 將 jieba 處理後的列表傳給 TfidfVectorizer\n",
        "        preprocessor=None,\n",
        "        token_pattern=None, # 禁用內建的 token_pattern\n",
        "        use_idf=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    except Exception as e:\n",
        "        return None, f\"TF-IDF 計算失敗，可能是文本量過少或分詞錯誤: {e}\"\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # 計算所有文檔的平均 TF-IDF 分數 (這裡的軸向必須是 0)\n",
        "    avg_tfidf_scores = tfidf_matrix.mean(axis=0).tolist()[0]\n",
        "\n",
        "    # 組合詞彙和分數\n",
        "    df_scores = pd.DataFrame({'詞彙': feature_names, '平均TFIDF得分': avg_tfidf_scores})\n",
        "\n",
        "    # 排序並選取前 N 個熱詞\n",
        "    top_keywords = df_scores.sort_values(by='平均TFIDF得分', ascending=False).head(TOP_N_KEYWORDS)\n",
        "\n",
        "    # 格式化分數\n",
        "    top_keywords['平均TFIDF得分'] = top_keywords['平均TFIDF得分'].apply(lambda x: f\"{x:.5f}\")\n",
        "\n",
        "    return top_keywords, None\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Gemini API 串接：洞察生成\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_insights(df_keywords, all_text):\n",
        "    \"\"\"串接 Gemini API 生成洞察摘要與結論。\"\"\"\n",
        "\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise gr.Error(\"❌ 錯誤：請在程式碼中填入有效的 `GEMINI_API_KEY`。\")\n",
        "\n",
        "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "    # 準備 Prompt\n",
        "    keyword_list = df_keywords.to_dict('records')\n",
        "    keyword_text = json.dumps(keyword_list, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # 將所有文本合併成一個大字符串（限制長度以避免超出上下文窗口）\n",
        "    # 這裡只取前 5000 字作為上下文參考\n",
        "    context_text = all_text[:5000]\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    你的任務是根據以下提供的原始文本內容和關鍵詞統計結果，生成一份專業分析。\n",
        "\n",
        "    ---\n",
        "    **統計關鍵詞列表 (TF-IDF Top {TOP_N_KEYWORDS}):**\n",
        "    {keyword_text}\n",
        "\n",
        "    **原始文本摘要 (作為上下文參考):**\n",
        "    ---\n",
        "    {context_text}\n",
        "    ---\n",
        "\n",
        "    請嚴格遵守以下輸出格式要求：\n",
        "    1.  **洞察摘要 (Insights):** 生成 5 句獨立的中文句子，每句提出一個有價值、基於數據的洞察或發現。請確保這 5 句清晰地列出。\n",
        "    2.  **結論 (Conclusion):** 生成一段完整的中文結論，字數必須控制在 **110 到 130 字** 之間，總結分析的結果並提出行動建議。\n",
        "    \"\"\"\n",
        "\n",
        "    # 系統指令：這次改為獨立的變數\n",
        "    system_instruction_text = \"你是一位精通中文分析的專家。你必須根據輸入數據，輸出一份包含「5 句洞察摘要」和「一段 110-130 字結論」的分析報告，不得包含其他冗餘的標題或解釋。\"\n",
        "\n",
        "    # 僅包含 user 內容 (這是正確的)\n",
        "    user_contents = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"parts\": [{\"text\": user_prompt}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "    # 執行 API 呼叫\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash',\n",
        "            contents=user_contents, # 僅 user 內容\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.3,\n",
        "                # 💡 關鍵修正：將系統指令移入 config 內，解決 'unexpected keyword argument' 錯誤\n",
        "                system_instruction=system_instruction_text\n",
        "            )\n",
        "            # 舊版本 SDK 不支援 system_instruction 作為頂層參數，故移除:\n",
        "            # system_instruction=system_instruction_text\n",
        "        )\n",
        "\n",
        "        # 簡易的格式化輸出 (解析模型的回覆)\n",
        "        insight_conclusion_text = response.text\n",
        "\n",
        "        # 嘗試解析出洞察和結論部分\n",
        "        parts = insight_conclusion_text.split('\\n')\n",
        "        insights = []\n",
        "        conclusion_lines = []\n",
        "\n",
        "        for line in parts:\n",
        "            if line.strip():\n",
        "                # 假設洞察是獨立的句子，或以列表符號開始\n",
        "                if re.match(r'^\\d+[\\.、]\\s*|^\\*\\s*|^\\-\\s*', line.strip()):\n",
        "                    insights.append(line.strip())\n",
        "                elif len(line.strip()) > 30: # 假設超過30字的連續文本是結論的一部分\n",
        "                    conclusion_lines.append(line.strip())\n",
        "\n",
        "        conclusion = \" \".join(conclusion_lines)\n",
        "\n",
        "        # 如果解析失敗，或者洞察不足 1 句，直接使用原始文本\n",
        "        if len(insights) < 1 or len(conclusion) < 50:\n",
        "             final_output = f\"**[AI 生成結果] (原始輸出)**\\n```\\n{insight_conclusion_text}\\n```\"\n",
        "        else:\n",
        "            final_output = \"### 💡 數據洞察摘要 (5 句)\\n\"\n",
        "            # 清理洞察句前的數字或符號\n",
        "            cleaned_insights = []\n",
        "            for i in insights:\n",
        "                 cleaned_insights.append(re.sub(r'^\\d+[\\.、]\\s*|^\\*\\s*|^\\-\\s*', '', i).strip())\n",
        "\n",
        "            final_output += \"\\n\".join([f\"- {i}\" for i in cleaned_insights[:5]]) # 只取前 5 句\n",
        "            final_output += \"\\n\\n### 📝 總結與建議 (約 120 字)\\n\"\n",
        "            final_output += conclusion\n",
        "\n",
        "        return final_output, \"✅ 成功\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # 顯示更友好的 API 錯誤訊息\n",
        "        raise gr.Error(f\"❌ Gemini API 呼叫失敗，請檢查 API Key 是否正確且服務是否已啟用。錯誤: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. 主流程控制函數 (Gradio 執行)\n",
        "# ==============================================================================\n",
        "\n",
        "def automated_analysis_flow(target_url, sheet_url_or_id):\n",
        "    \"\"\"\n",
        "    整個自動化流程的主控制函數。\n",
        "    \"\"\"\n",
        "\n",
        "    # 輸出變數初始化\n",
        "    status_log = \"\"\n",
        "    top_keywords_df = pd.DataFrame({'詞彙': ['N/A'], '平均TFIDF得分': ['N/A']})\n",
        "    gemini_output = \"等待執行結果...\"\n",
        "\n",
        "    if not target_url or not sheet_url_or_id:\n",
        "        raise gr.Error(\"請輸入目標 URL 和 Google Sheet ID/URL。\")\n",
        "\n",
        "    try:\n",
        "        # --- 步驟 1: 爬蟲與寫入 Google Sheets ---\n",
        "        status_log += f\"1. 開始爬取 URL: {target_url}\\n\"\n",
        "        raw_data_df = scrape_data(target_url)\n",
        "\n",
        "        if raw_data_df.empty or raw_data_df['內容'].iloc[0].startswith((\"連線錯誤\", \"爬蟲解析錯誤\", \"無法提取\")):\n",
        "            error_message = raw_data_df['內容'].iloc[0]\n",
        "            status_log += f\"❌ 爬蟲或數據提取失敗: {error_message}\\n\"\n",
        "            raise gr.Error(f\"爬蟲失敗: {error_message}\")\n",
        "\n",
        "        status_log += f\"   ✅ 爬取成功，獲得 {len(raw_data_df)} 筆數據。\\n\"\n",
        "\n",
        "        # 初始化 GS\n",
        "        gc = init_gspread()\n",
        "        status_log += f\"   ✅ Google Sheets 服務帳戶授權成功。\\n\"\n",
        "\n",
        "        # 開啟試算表\n",
        "        spreadsheet = gc.open_by_url(sheet_url_or_id) if sheet_url_or_id.startswith('http') else gc.open_by_key(sheet_url_or_id)\n",
        "        status_log += f\"   ✅ 成功開啟試算表: {spreadsheet.title}\\n\"\n",
        "\n",
        "        # 寫入原始數據工作表\n",
        "        data_ws = get_or_create_worksheet(spreadsheet, DATA_WORKSHEET_TITLE, [\"標題\", \"內容\"])\n",
        "        data_ws.clear()\n",
        "        data_ws.update([raw_data_df.columns.values.tolist()] + raw_data_df.values.tolist())\n",
        "        status_log += f\"   ✅ 原始數據已寫入到 [{DATA_WORKSHEET_TITLE}]。\\n\"\n",
        "\n",
        "        # --- 步驟 2: TF-IDF 詞頻與關鍵字統計 ---\n",
        "        status_log += \"2. 開始執行 TF-IDF 文本分析...\\n\"\n",
        "        top_keywords_df, error = perform_analysis(raw_data_df)\n",
        "\n",
        "        if error:\n",
        "            status_log += f\"❌ TF-IDF 分析失敗: {error}\\n\"\n",
        "            raise gr.Error(f\"TF-IDF 分析失敗: {error}\")\n",
        "\n",
        "        status_log += f\"   ✅ TF-IDF 分析完成，提取前 {TOP_N_KEYWORDS} 熱詞。\\n\"\n",
        "\n",
        "        # 將統計結果回寫到 Google Sheets\n",
        "        stats_ws = get_or_create_worksheet(spreadsheet, STATS_WORKSHEET_TITLE, [\"詞彙\", \"平均TFIDF得分\"])\n",
        "        stats_ws.clear()\n",
        "        stats_ws.update([top_keywords_df.columns.values.tolist()] + top_keywords_df.values.tolist())\n",
        "        status_log += f\"   ✅ 統計結果已回寫到 [{STATS_WORKSHEET_TITLE}]。\\n\"\n",
        "\n",
        "        # --- 步驟 3: 串接 Gemini API 生成洞察摘要與結論 ---\n",
        "        status_log += \"3. 串接 Gemini API，生成洞察與結論...\\n\"\n",
        "        all_text_content = \"\\n\".join(raw_data_df['內容'].astype(str).tolist())\n",
        "\n",
        "        gemini_output, gemini_status = generate_insights(top_keywords_df, all_text_content)\n",
        "\n",
        "        status_log += f\"   {gemini_status} Gemini 洞察生成完成。\\n\"\n",
        "        status_log += \"--- 執行流程結束 ---\"\n",
        "\n",
        "        return status_log, top_keywords_df, gemini_output\n",
        "\n",
        "    except gr.Error as ge:\n",
        "        # Gradio.Error 會被自動顯示，這裡只更新 Log\n",
        "        status_log += str(ge).replace(\"gr.Error: \", \"\")\n",
        "        return status_log, top_keywords_df, \"執行中斷，請查看 Log 資訊。\"\n",
        "    except Exception as e:\n",
        "        # 其他未預期的致命錯誤\n",
        "        error_msg = f\"❌ 致命錯誤發生: {e}\"\n",
        "        status_log += error_msg\n",
        "        raise gr.Error(f\"致命錯誤: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Gradio 介面設計\n",
        "# ==============================================================================\n",
        "\n",
        "# 配置 Gradio 介面\n",
        "with gr.Blocks(title=\"全自動化文本洞察生成器\") as demo:\n",
        "    gr.Markdown(\"# 🤖 全自動文本洞察生成器 (Gradio + Gemini + Google Sheets)\")\n",
        "    gr.Markdown(\n",
        "        \"此應用程式自動執行 **爬蟲 → 寫入 Sheets → 詞頻分析 → 回寫 Sheets → Gemini 洞察生成** 的完整流程。\\n\"\n",
        "        \"**🚨 重要：請務必將 JSON 憑證檔案命名為 `service_account_key.json` 並上傳！**\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # 輸入區\n",
        "        with gr.Column(scale=1):\n",
        "            target_url_input = gr.Textbox(\n",
        "                label=\"Step 1: 爬蟲目標 URL (請輸入一個單頁文章連結)\",\n",
        "                value=\"https://www.ettoday.net/news/20241026/2847285.htm\",\n",
        "                placeholder=\"例如: https://example.com/news/1\"\n",
        "            )\n",
        "            sheet_id_input = gr.Textbox(\n",
        "                label=\"Step 2: Google Sheet URL 或 ID (請務必填寫)\",\n",
        "                value=\"https://docs.google.com/spreadsheets/d/1cFttfMP93Bdzhtcqid06xTCqqZ0h5lrFODHXk8Se5R0/edit?gid=1806628729#gid=1806628729\",\n",
        "                placeholder=\"https://docs.google.com/spreadsheets/d/...\"\n",
        "            )\n",
        "            run_button = gr.Button(\"🚀 一鍵執行分析流程\", variant=\"primary\")\n",
        "\n",
        "        # 狀態輸出區\n",
        "        with gr.Column(scale=1):\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"流程執行狀態紀錄\",\n",
        "                lines=10,\n",
        "                interactive=False,\n",
        "                placeholder=\"等待執行...\"\n",
        "            )\n",
        "\n",
        "    # 結果輸出區\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"📊 關鍵詞統計結果 (回寫到 Google Sheet)\"):\n",
        "            keywords_output = gr.Dataframe(\n",
        "                label=f\"前 {TOP_N_KEYWORDS} 個熱詞及其 TF-IDF 平均得分 (已回寫到 Sheets)\",\n",
        "                headers=[\"詞彙\", \"平均TFIDF得分\"],\n",
        "                datatype=[\"str\", \"str\"],\n",
        "                wrap=True\n",
        "            )\n",
        "        with gr.TabItem(\"🤖 Gemini 洞察摘要與結論\"):\n",
        "            gemini_insight_output = gr.Markdown(\n",
        "                label=\"AI 生成的洞察摘要與 120 字結論\",\n",
        "                value=\"Gemini API 輸出結果將顯示在這裡...\"\n",
        "            )\n",
        "\n",
        "    # 綁定事件\n",
        "    run_button.click(\n",
        "        fn=automated_analysis_flow,\n",
        "        inputs=[target_url_input, sheet_id_input],\n",
        "        outputs=[status_output, keywords_output, gemini_insight_output]\n",
        "    )\n",
        "\n",
        "# 啟動 Gradio 服務\n",
        "if __name__ == \"__main__\":\n",
        "    # 載入 jieba 詞典，準備分詞\n",
        "    jieba.initialize()\n",
        "    print(\"Jieba 分詞庫已初始化。\")\n",
        "\n",
        "    demo.launch()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jieba 分詞庫已初始化。\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a930accd6b6b8cd751.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a930accd6b6b8cd751.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "nKyRl2_xS3lc",
        "outputId": "245db946-6763-4367-a842-c20bfe4e1b97"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}