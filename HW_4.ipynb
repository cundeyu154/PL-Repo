{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cundeyu154/PL-Repo/blob/main/HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-genai pandas numpy requests beautifulsoup4 scikit-learn jieba gradio gspread"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WSNNAhLUJrp",
        "outputId": "8703af17-54a9-4a7e-e9c0-f453fe7960cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.45.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (6.2.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread) (1.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba\n",
        "import gradio as gr\n",
        "import gspread\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. é…ç½®èˆ‡å¸¸é‡ (è«‹æ›¿æ›ç‚ºæ‚¨çš„å¯¦éš›å€¼ï¼)\n",
        "# ==============================================================================\n",
        "GEMINI_API_KEY = \"AIzaSyDN-LWFIop1hMP2J3lKLN8hU06ZOb3tkvM\"\n",
        "\n",
        "GS_CREDENTIALS_PATH = \"service_account_key.json\"\n",
        "\n",
        "# Google Sheet è¨­ç½®\n",
        "SHEET_NAME = \"æ–‡æœ¬åˆ†æè‡ªå‹•åŒ–å°ˆæ¡ˆ\"\n",
        "DATA_WORKSHEET_TITLE = \"åŸå§‹æ•¸æ“š\" # çˆ¬èŸ²çµæœå­˜æ”¾çš„å·¥ä½œè¡¨åç¨±\n",
        "STATS_WORKSHEET_TITLE = \"çµ±è¨ˆçµæœ\" # é—œéµè©çµ±è¨ˆçµæœå­˜æ”¾çš„å·¥ä½œè¡¨åç¨±\n",
        "TOP_N_KEYWORDS = 20 # è¦æå–çš„ç†±è©æ•¸é‡\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. è¼”åŠ©å‡½æ•¸ï¼šGoogle Sheets è™•ç†\n",
        "# ==============================================================================\n",
        "\n",
        "def init_gspread():\n",
        "    \"\"\"åˆå§‹åŒ– gspread å®¢æˆ¶ç«¯ä¸¦æˆæ¬Š.\"\"\"\n",
        "    try:\n",
        "        # ä½¿ç”¨æœå‹™å¸³æˆ¶é‡‘é‘°æª”æ¡ˆé€²è¡Œæˆæ¬Š\n",
        "        gc = gspread.service_account(filename=GS_CREDENTIALS_PATH)\n",
        "        return gc\n",
        "    except Exception as e:\n",
        "        # åœ¨ Gradio ä»‹é¢ä¸­æ‹‹å‡ºéŒ¯èª¤ï¼Œè®“ä½¿ç”¨è€…çœ‹åˆ°\n",
        "        raise gr.Error(f\"Google Sheets æˆæ¬Šå¤±æ•—ã€‚è«‹æª¢æŸ¥è¨­å®šã€‚éŒ¯èª¤: {e}\")\n",
        "\n",
        "def get_or_create_worksheet(spreadsheet, title, header):\n",
        "    \"\"\"å–å¾—æˆ–å‰µå»ºæŒ‡å®šåç¨±çš„å·¥ä½œè¡¨ã€‚\"\"\"\n",
        "    try:\n",
        "        worksheet = spreadsheet.worksheet(title)\n",
        "        return worksheet\n",
        "    except gspread.WorksheetNotFound:\n",
        "        # å¦‚æœå·¥ä½œè¡¨ä¸å­˜åœ¨ï¼Œå‰‡å‰µå»ºå®ƒä¸¦å¯«å…¥æ¨™é¡Œ\n",
        "        worksheet = spreadsheet.add_worksheet(title=title, rows=\"100\", cols=\"20\")\n",
        "        worksheet.update('A1', [header])\n",
        "        return worksheet\n",
        "    except Exception as e:\n",
        "        raise gr.Error(f\"æ“ä½œå·¥ä½œè¡¨ {title} å¤±æ•—: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. æ•¸æ“šæ”¶é›†ï¼šçˆ¬èŸ²åŠŸèƒ½ (å·²å¼·åŒ–ç‚ºå¯¦éš›æå–)\n",
        "# ==============================================================================\n",
        "\n",
        "def scrape_data(target_url):\n",
        "    \"\"\"\n",
        "    å¾å–®ä¸€ URL çˆ¬å–æ–‡ç« ä¸»é«”æ–‡æœ¬çš„å¼·åŒ–ç‰ˆå‡½æ•¸ã€‚\n",
        "    å®ƒå˜—è©¦ä½¿ç”¨å¸¸è¦‹çš„ CSS é¸æ“‡å™¨ä¾†å®šä½ä¸»è¦æ–‡ç« å…§å®¹ã€‚\n",
        "    \"\"\"\n",
        "    url = target_url\n",
        "    if not url.startswith(('http://', 'https://')):\n",
        "        url = 'https://' + url\n",
        "\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        response.raise_for_status() # æª¢æŸ¥ HTTP éŒ¯èª¤\n",
        "\n",
        "        # é€™è£¡è¨­ç½®ç·¨ç¢¼è™•ç†ï¼Œé¿å…ä¸­æ–‡äº‚ç¢¼\n",
        "        response.encoding = response.apparent_encoding\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # å¼·åŒ–ï¼šå˜—è©¦ä½¿ç”¨æ›´ç²¾ç¢ºçš„é¸æ“‡å™¨ä¾†å®šä½æ–‡ç« ä¸»é«”\n",
        "        content_selectors = [\n",
        "            \"article\",\n",
        "            \".article-content\",\n",
        "            \".post-body\",\n",
        "            \".entry-content\",\n",
        "            \".main-content\",\n",
        "            \"#content-body\", # é‡å°ç‰¹å®šç¶²ç«™çµæ§‹æ·»åŠ \n",
        "        ]\n",
        "\n",
        "        main_content_element = None\n",
        "        for selector in content_selectors:\n",
        "            # å˜—è©¦ä½¿ç”¨ CSS é¸æ“‡å™¨æŸ¥æ‰¾\n",
        "            main_content_element = soup.select_one(selector)\n",
        "            if main_content_element:\n",
        "                break\n",
        "\n",
        "        paragraphs = []\n",
        "        if main_content_element:\n",
        "            # å¦‚æœæ‰¾åˆ°ä¸»é«”å…ƒç´ ï¼Œåªå¾è©²å…ƒç´ å…§æå–æ‰€æœ‰æ®µè½\n",
        "            paragraphs = [p.get_text().strip() for p in main_content_element.find_all('p')]\n",
        "        else:\n",
        "            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç²¾ç¢ºçš„ä¸»é«”å…ƒç´ ï¼Œå‰‡é€€å›åˆ°å…¨é é¢çš„ <p> æ¨™ç±¤\n",
        "            paragraphs = [p.get_text().strip() for p in soup.find_all('p')]\n",
        "\n",
        "        # éæ¿¾ç©ºè¡Œä¸¦åˆä½µ\n",
        "        full_text = '\\n'.join(filter(None, paragraphs))\n",
        "\n",
        "        # å¦‚æœç„¡æ³•æå–å¤§é‡æ–‡æœ¬ï¼Œå¯èƒ½æ˜¯ç¶²ç«™çµæ§‹è¤‡é›œï¼Œæˆ–é é¢éä¸»è¦æ–‡ç« \n",
        "        if not full_text or len(full_text) < 100:\n",
        "            # å˜—è©¦æŠ“å– body è£¡çš„æ‰€æœ‰æ–‡æœ¬ä½œç‚ºæœ€çµ‚å‚™é¸\n",
        "            body_text = soup.body.get_text(separator='\\n', strip=True) if soup.body else \"\"\n",
        "            if len(body_text) > 100:\n",
        "                 full_text = body_text\n",
        "            else:\n",
        "                return pd.DataFrame({\"æ¨™é¡Œ\": [\"ç„¡æ³•æå–ä¸»é«”æ–‡æœ¬\"], \"å…§å®¹\": [\"è«‹æª¢æŸ¥ URLã€ç¶²ç«™çµæ§‹æˆ–çˆ¬èŸ²é‚è¼¯ã€‚\"]})\n",
        "\n",
        "        # æ¨¡æ“¬å–®ç­†æ•¸æ“šçš„ DataFrame çµæ§‹\n",
        "        data = pd.DataFrame({\n",
        "            \"æ¨™é¡Œ\": [soup.title.string.strip() if soup.title else \"ç„¡æ¨™é¡Œ\"],\n",
        "            \"å…§å®¹\": [full_text]\n",
        "        })\n",
        "        return data\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        error_msg = f\"é€£ç·šéŒ¯èª¤: {e.__class__.__name__} - {e}\"\n",
        "        return pd.DataFrame({\"æ¨™é¡Œ\": [error_msg], \"å…§å®¹\": [error_msg]})\n",
        "    except Exception as e:\n",
        "        error_msg = f\"çˆ¬èŸ²è§£æéŒ¯èª¤: {e.__class__.__name__} - {e}\"\n",
        "        return pd.DataFrame({\"æ¨™é¡Œ\": [error_msg], \"å…§å®¹\": [error_msg]})\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. æ–‡æœ¬åˆ†æï¼šTF-IDF èˆ‡é—œéµè©æå–\n",
        "# ==============================================================================\n",
        "\n",
        "def chinese_tokenizer(text):\n",
        "    \"\"\"ä½¿ç”¨ jieba é€²è¡Œä¸­æ–‡åˆ†è©ã€‚\"\"\"\n",
        "    # æ“´å……æ›´å®Œæ•´çš„ä¸­æ–‡åœç”¨è©åˆ—è¡¨\n",
        "    # é€™è£¡åªä½¿ç”¨ä¸€å€‹éå¸¸ç°¡åŒ–çš„åœç”¨è©åˆ—è¡¨ï¼Œå¯¦éš›æ‡‰ç”¨ä¸­è«‹æ“´å……\n",
        "    simple_stopwords = {'çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'å’Œ', 'ä½†', 'ä¹Ÿ', 'è€Œ', 'é€™', 'é‚£', 'ä¸€å€‹', 'ä¸€ç¨®', 'å¯ä»¥', 'èƒ½å¤ ', 'å› ç‚º', 'æ‰€ä»¥', 'é€™æ¨£', 'é‚£æ¨£', 'åœ¨', 'ä¸Š', 'ä¸‹', 'ä¸­', 'ä¹‹', 'æ‰€', 'å‰‡', 'ç­‰', 'èˆ‡', 'æˆ–'}\n",
        "\n",
        "    # ä½¿ç”¨ jieba.cut é€²è¡Œç²¾ç¢ºåˆ†è©\n",
        "    words = jieba.cut(text, cut_all=False)\n",
        "\n",
        "    # éæ¿¾åœç”¨è©ã€å–®å­—è©ï¼ˆé€šå¸¸ç‚ºåŠ©è©/æ¨™é»ï¼‰ã€æ•¸å­—å’Œç©ºç™½\n",
        "    filtered_words = [\n",
        "        word.lower()\n",
        "        for word in words\n",
        "        if word.strip() and\n",
        "           len(word.strip()) > 1 and\n",
        "           word.lower() not in simple_stopwords and\n",
        "           not word.strip().isdigit()\n",
        "    ]\n",
        "\n",
        "    # å¿…é ˆè¿”å›ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²ï¼Œé€™æ˜¯ TfidfVectorizer æœŸæœ›çš„æ ¼å¼\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def dummy_tokenizer(text):\n",
        "    \"\"\"TfidfVectorizer éœ€è¦ä¸€å€‹æ¥å—æ–‡æœ¬ä¸¦è¿”å›åˆ†è©å¾Œçš„ç©ºæ ¼åˆ†éš”å­—ä¸²çš„ callableã€‚\"\"\"\n",
        "    return chinese_tokenizer(text).split(' ')\n",
        "\n",
        "def perform_analysis(df):\n",
        "    \"\"\"åŸ·è¡Œ TF-IDF è¨ˆç®—ï¼Œæå–ç†±è©ã€‚\"\"\"\n",
        "    if df.empty or 'å…§å®¹' not in df.columns:\n",
        "        return None, \"æ•¸æ“šæ¡†ç„¡å…§å®¹æˆ–æ ¼å¼éŒ¯èª¤ã€‚\"\n",
        "\n",
        "    corpus = df['å…§å®¹'].astype(str).tolist()\n",
        "\n",
        "    # ä½¿ç”¨ TfidfVectorizer é€²è¡Œ TF-IDF è¨ˆç®—\n",
        "    # æˆ‘å€‘å°‡ preprocessor å’Œ tokenizer è¨­ç‚º Noneï¼Œè®“ token_pattern è™•ç†ï¼Œ\n",
        "    # ä½†ç‚ºäº†ä¸­æ–‡ç²¾ç¢ºåˆ†è©ï¼Œæˆ‘å€‘å¿…é ˆç”¨è‡ªå·±çš„åˆ†è©å‡½æ•¸\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        tokenizer=dummy_tokenizer, # ä½¿ç”¨ dummy_tokenizer å°‡ jieba è™•ç†å¾Œçš„åˆ—è¡¨å‚³çµ¦ TfidfVectorizer\n",
        "        preprocessor=None,\n",
        "        token_pattern=None, # ç¦ç”¨å…§å»ºçš„ token_pattern\n",
        "        use_idf=True\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "    except Exception as e:\n",
        "        return None, f\"TF-IDF è¨ˆç®—å¤±æ•—ï¼Œå¯èƒ½æ˜¯æ–‡æœ¬é‡éå°‘æˆ–åˆ†è©éŒ¯èª¤: {e}\"\n",
        "\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # è¨ˆç®—æ‰€æœ‰æ–‡æª”çš„å¹³å‡ TF-IDF åˆ†æ•¸ (é€™è£¡çš„è»¸å‘å¿…é ˆæ˜¯ 0)\n",
        "    avg_tfidf_scores = tfidf_matrix.mean(axis=0).tolist()[0]\n",
        "\n",
        "    # çµ„åˆè©å½™å’Œåˆ†æ•¸\n",
        "    df_scores = pd.DataFrame({'è©å½™': feature_names, 'å¹³å‡TFIDFå¾—åˆ†': avg_tfidf_scores})\n",
        "\n",
        "    # æ’åºä¸¦é¸å–å‰ N å€‹ç†±è©\n",
        "    top_keywords = df_scores.sort_values(by='å¹³å‡TFIDFå¾—åˆ†', ascending=False).head(TOP_N_KEYWORDS)\n",
        "\n",
        "    # æ ¼å¼åŒ–åˆ†æ•¸\n",
        "    top_keywords['å¹³å‡TFIDFå¾—åˆ†'] = top_keywords['å¹³å‡TFIDFå¾—åˆ†'].apply(lambda x: f\"{x:.5f}\")\n",
        "\n",
        "    return top_keywords, None\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. Gemini API ä¸²æ¥ï¼šæ´å¯Ÿç”Ÿæˆ\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_insights(df_keywords, all_text):\n",
        "    \"\"\"ä¸²æ¥ Gemini API ç”Ÿæˆæ´å¯Ÿæ‘˜è¦èˆ‡çµè«–ã€‚\"\"\"\n",
        "\n",
        "    if not GEMINI_API_KEY or GEMINI_API_KEY == \"YOUR_GEMINI_API_KEY_HERE\":\n",
        "        raise gr.Error(\"âŒ éŒ¯èª¤ï¼šè«‹åœ¨ç¨‹å¼ç¢¼ä¸­å¡«å…¥æœ‰æ•ˆçš„ `GEMINI_API_KEY`ã€‚\")\n",
        "\n",
        "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "    # æº–å‚™ Prompt\n",
        "    keyword_list = df_keywords.to_dict('records')\n",
        "    keyword_text = json.dumps(keyword_list, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # å°‡æ‰€æœ‰æ–‡æœ¬åˆä½µæˆä¸€å€‹å¤§å­—ç¬¦ä¸²ï¼ˆé™åˆ¶é•·åº¦ä»¥é¿å…è¶…å‡ºä¸Šä¸‹æ–‡çª—å£ï¼‰\n",
        "    # é€™è£¡åªå–å‰ 5000 å­—ä½œç‚ºä¸Šä¸‹æ–‡åƒè€ƒ\n",
        "    context_text = all_text[:5000]\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šä»¥ä¸‹æä¾›çš„åŸå§‹æ–‡æœ¬å…§å®¹å’Œé—œéµè©çµ±è¨ˆçµæœï¼Œç”Ÿæˆä¸€ä»½å°ˆæ¥­åˆ†æã€‚\n",
        "\n",
        "    ---\n",
        "    **çµ±è¨ˆé—œéµè©åˆ—è¡¨ (TF-IDF Top {TOP_N_KEYWORDS}):**\n",
        "    {keyword_text}\n",
        "\n",
        "    **åŸå§‹æ–‡æœ¬æ‘˜è¦ (ä½œç‚ºä¸Šä¸‹æ–‡åƒè€ƒ):**\n",
        "    ---\n",
        "    {context_text}\n",
        "    ---\n",
        "\n",
        "    è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¼¸å‡ºæ ¼å¼è¦æ±‚ï¼š\n",
        "    1.  **æ´å¯Ÿæ‘˜è¦ (Insights):** ç”Ÿæˆ 5 å¥ç¨ç«‹çš„ä¸­æ–‡å¥å­ï¼Œæ¯å¥æå‡ºä¸€å€‹æœ‰åƒ¹å€¼ã€åŸºæ–¼æ•¸æ“šçš„æ´å¯Ÿæˆ–ç™¼ç¾ã€‚è«‹ç¢ºä¿é€™ 5 å¥æ¸…æ™°åœ°åˆ—å‡ºã€‚\n",
        "    2.  **çµè«– (Conclusion):** ç”Ÿæˆä¸€æ®µå®Œæ•´çš„ä¸­æ–‡çµè«–ï¼Œå­—æ•¸å¿…é ˆæ§åˆ¶åœ¨ **110 åˆ° 130 å­—** ä¹‹é–“ï¼Œç¸½çµåˆ†æçš„çµæœä¸¦æå‡ºè¡Œå‹•å»ºè­°ã€‚\n",
        "    \"\"\"\n",
        "\n",
        "    # ç³»çµ±æŒ‡ä»¤ï¼šé€™æ¬¡æ”¹ç‚ºç¨ç«‹çš„è®Šæ•¸\n",
        "    system_instruction_text = \"ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­æ–‡åˆ†æçš„å°ˆå®¶ã€‚ä½ å¿…é ˆæ ¹æ“šè¼¸å…¥æ•¸æ“šï¼Œè¼¸å‡ºä¸€ä»½åŒ…å«ã€Œ5 å¥æ´å¯Ÿæ‘˜è¦ã€å’Œã€Œä¸€æ®µ 110-130 å­—çµè«–ã€çš„åˆ†æå ±å‘Šï¼Œä¸å¾—åŒ…å«å…¶ä»–å†—é¤˜çš„æ¨™é¡Œæˆ–è§£é‡‹ã€‚\"\n",
        "\n",
        "    # åƒ…åŒ…å« user å…§å®¹ (é€™æ˜¯æ­£ç¢ºçš„)\n",
        "    user_contents = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"parts\": [{\"text\": user_prompt}]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "    # åŸ·è¡Œ API å‘¼å«\n",
        "    try:\n",
        "        response = client.models.generate_content(\n",
        "            model='gemini-2.5-flash',\n",
        "            contents=user_contents, # åƒ… user å…§å®¹\n",
        "            config=types.GenerateContentConfig(\n",
        "                temperature=0.3,\n",
        "                # ğŸ’¡ é—œéµä¿®æ­£ï¼šå°‡ç³»çµ±æŒ‡ä»¤ç§»å…¥ config å…§ï¼Œè§£æ±º 'unexpected keyword argument' éŒ¯èª¤\n",
        "                system_instruction=system_instruction_text\n",
        "            )\n",
        "            # èˆŠç‰ˆæœ¬ SDK ä¸æ”¯æ´ system_instruction ä½œç‚ºé ‚å±¤åƒæ•¸ï¼Œæ•…ç§»é™¤:\n",
        "            # system_instruction=system_instruction_text\n",
        "        )\n",
        "\n",
        "        # ç°¡æ˜“çš„æ ¼å¼åŒ–è¼¸å‡º (è§£ææ¨¡å‹çš„å›è¦†)\n",
        "        insight_conclusion_text = response.text\n",
        "\n",
        "        # å˜—è©¦è§£æå‡ºæ´å¯Ÿå’Œçµè«–éƒ¨åˆ†\n",
        "        parts = insight_conclusion_text.split('\\n')\n",
        "        insights = []\n",
        "        conclusion_lines = []\n",
        "\n",
        "        for line in parts:\n",
        "            if line.strip():\n",
        "                # å‡è¨­æ´å¯Ÿæ˜¯ç¨ç«‹çš„å¥å­ï¼Œæˆ–ä»¥åˆ—è¡¨ç¬¦è™Ÿé–‹å§‹\n",
        "                if re.match(r'^\\d+[\\.ã€]\\s*|^\\*\\s*|^\\-\\s*', line.strip()):\n",
        "                    insights.append(line.strip())\n",
        "                elif len(line.strip()) > 30: # å‡è¨­è¶…é30å­—çš„é€£çºŒæ–‡æœ¬æ˜¯çµè«–çš„ä¸€éƒ¨åˆ†\n",
        "                    conclusion_lines.append(line.strip())\n",
        "\n",
        "        conclusion = \" \".join(conclusion_lines)\n",
        "\n",
        "        # å¦‚æœè§£æå¤±æ•—ï¼Œæˆ–è€…æ´å¯Ÿä¸è¶³ 1 å¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æœ¬\n",
        "        if len(insights) < 1 or len(conclusion) < 50:\n",
        "             final_output = f\"**[AI ç”Ÿæˆçµæœ] (åŸå§‹è¼¸å‡º)**\\n```\\n{insight_conclusion_text}\\n```\"\n",
        "        else:\n",
        "            final_output = \"### ğŸ’¡ æ•¸æ“šæ´å¯Ÿæ‘˜è¦ (5 å¥)\\n\"\n",
        "            # æ¸…ç†æ´å¯Ÿå¥å‰çš„æ•¸å­—æˆ–ç¬¦è™Ÿ\n",
        "            cleaned_insights = []\n",
        "            for i in insights:\n",
        "                 cleaned_insights.append(re.sub(r'^\\d+[\\.ã€]\\s*|^\\*\\s*|^\\-\\s*', '', i).strip())\n",
        "\n",
        "            final_output += \"\\n\".join([f\"- {i}\" for i in cleaned_insights[:5]]) # åªå–å‰ 5 å¥\n",
        "            final_output += \"\\n\\n### ğŸ“ ç¸½çµèˆ‡å»ºè­° (ç´„ 120 å­—)\\n\"\n",
        "            final_output += conclusion\n",
        "\n",
        "        return final_output, \"âœ… æˆåŠŸ\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # é¡¯ç¤ºæ›´å‹å¥½çš„ API éŒ¯èª¤è¨Šæ¯\n",
        "        raise gr.Error(f\"âŒ Gemini API å‘¼å«å¤±æ•—ï¼Œè«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢ºä¸”æœå‹™æ˜¯å¦å·²å•Ÿç”¨ã€‚éŒ¯èª¤: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. ä¸»æµç¨‹æ§åˆ¶å‡½æ•¸ (Gradio åŸ·è¡Œ)\n",
        "# ==============================================================================\n",
        "\n",
        "def automated_analysis_flow(target_url, sheet_url_or_id):\n",
        "    \"\"\"\n",
        "    æ•´å€‹è‡ªå‹•åŒ–æµç¨‹çš„ä¸»æ§åˆ¶å‡½æ•¸ã€‚\n",
        "    \"\"\"\n",
        "\n",
        "    # è¼¸å‡ºè®Šæ•¸åˆå§‹åŒ–\n",
        "    status_log = \"\"\n",
        "    top_keywords_df = pd.DataFrame({'è©å½™': ['N/A'], 'å¹³å‡TFIDFå¾—åˆ†': ['N/A']})\n",
        "    gemini_output = \"ç­‰å¾…åŸ·è¡Œçµæœ...\"\n",
        "\n",
        "    if not target_url or not sheet_url_or_id:\n",
        "        raise gr.Error(\"è«‹è¼¸å…¥ç›®æ¨™ URL å’Œ Google Sheet ID/URLã€‚\")\n",
        "\n",
        "    try:\n",
        "        # --- æ­¥é©Ÿ 1: çˆ¬èŸ²èˆ‡å¯«å…¥ Google Sheets ---\n",
        "        status_log += f\"1. é–‹å§‹çˆ¬å– URL: {target_url}\\n\"\n",
        "        raw_data_df = scrape_data(target_url)\n",
        "\n",
        "        if raw_data_df.empty or raw_data_df['å…§å®¹'].iloc[0].startswith((\"é€£ç·šéŒ¯èª¤\", \"çˆ¬èŸ²è§£æéŒ¯èª¤\", \"ç„¡æ³•æå–\")):\n",
        "            error_message = raw_data_df['å…§å®¹'].iloc[0]\n",
        "            status_log += f\"âŒ çˆ¬èŸ²æˆ–æ•¸æ“šæå–å¤±æ•—: {error_message}\\n\"\n",
        "            raise gr.Error(f\"çˆ¬èŸ²å¤±æ•—: {error_message}\")\n",
        "\n",
        "        status_log += f\"   âœ… çˆ¬å–æˆåŠŸï¼Œç²å¾— {len(raw_data_df)} ç­†æ•¸æ“šã€‚\\n\"\n",
        "\n",
        "        # åˆå§‹åŒ– GS\n",
        "        gc = init_gspread()\n",
        "        status_log += f\"   âœ… Google Sheets æœå‹™å¸³æˆ¶æˆæ¬ŠæˆåŠŸã€‚\\n\"\n",
        "\n",
        "        # é–‹å•Ÿè©¦ç®—è¡¨\n",
        "        spreadsheet = gc.open_by_url(sheet_url_or_id) if sheet_url_or_id.startswith('http') else gc.open_by_key(sheet_url_or_id)\n",
        "        status_log += f\"   âœ… æˆåŠŸé–‹å•Ÿè©¦ç®—è¡¨: {spreadsheet.title}\\n\"\n",
        "\n",
        "        # å¯«å…¥åŸå§‹æ•¸æ“šå·¥ä½œè¡¨\n",
        "        data_ws = get_or_create_worksheet(spreadsheet, DATA_WORKSHEET_TITLE, [\"æ¨™é¡Œ\", \"å…§å®¹\"])\n",
        "        data_ws.clear()\n",
        "        data_ws.update([raw_data_df.columns.values.tolist()] + raw_data_df.values.tolist())\n",
        "        status_log += f\"   âœ… åŸå§‹æ•¸æ“šå·²å¯«å…¥åˆ° [{DATA_WORKSHEET_TITLE}]ã€‚\\n\"\n",
        "\n",
        "        # --- æ­¥é©Ÿ 2: TF-IDF è©é »èˆ‡é—œéµå­—çµ±è¨ˆ ---\n",
        "        status_log += \"2. é–‹å§‹åŸ·è¡Œ TF-IDF æ–‡æœ¬åˆ†æ...\\n\"\n",
        "        top_keywords_df, error = perform_analysis(raw_data_df)\n",
        "\n",
        "        if error:\n",
        "            status_log += f\"âŒ TF-IDF åˆ†æå¤±æ•—: {error}\\n\"\n",
        "            raise gr.Error(f\"TF-IDF åˆ†æå¤±æ•—: {error}\")\n",
        "\n",
        "        status_log += f\"   âœ… TF-IDF åˆ†æå®Œæˆï¼Œæå–å‰ {TOP_N_KEYWORDS} ç†±è©ã€‚\\n\"\n",
        "\n",
        "        # å°‡çµ±è¨ˆçµæœå›å¯«åˆ° Google Sheets\n",
        "        stats_ws = get_or_create_worksheet(spreadsheet, STATS_WORKSHEET_TITLE, [\"è©å½™\", \"å¹³å‡TFIDFå¾—åˆ†\"])\n",
        "        stats_ws.clear()\n",
        "        stats_ws.update([top_keywords_df.columns.values.tolist()] + top_keywords_df.values.tolist())\n",
        "        status_log += f\"   âœ… çµ±è¨ˆçµæœå·²å›å¯«åˆ° [{STATS_WORKSHEET_TITLE}]ã€‚\\n\"\n",
        "\n",
        "        # --- æ­¥é©Ÿ 3: ä¸²æ¥ Gemini API ç”Ÿæˆæ´å¯Ÿæ‘˜è¦èˆ‡çµè«– ---\n",
        "        status_log += \"3. ä¸²æ¥ Gemini APIï¼Œç”Ÿæˆæ´å¯Ÿèˆ‡çµè«–...\\n\"\n",
        "        all_text_content = \"\\n\".join(raw_data_df['å…§å®¹'].astype(str).tolist())\n",
        "\n",
        "        gemini_output, gemini_status = generate_insights(top_keywords_df, all_text_content)\n",
        "\n",
        "        status_log += f\"   {gemini_status} Gemini æ´å¯Ÿç”Ÿæˆå®Œæˆã€‚\\n\"\n",
        "        status_log += \"--- åŸ·è¡Œæµç¨‹çµæŸ ---\"\n",
        "\n",
        "        return status_log, top_keywords_df, gemini_output\n",
        "\n",
        "    except gr.Error as ge:\n",
        "        # Gradio.Error æœƒè¢«è‡ªå‹•é¡¯ç¤ºï¼Œé€™è£¡åªæ›´æ–° Log\n",
        "        status_log += str(ge).replace(\"gr.Error: \", \"\")\n",
        "        return status_log, top_keywords_df, \"åŸ·è¡Œä¸­æ–·ï¼Œè«‹æŸ¥çœ‹ Log è³‡è¨Šã€‚\"\n",
        "    except Exception as e:\n",
        "        # å…¶ä»–æœªé æœŸçš„è‡´å‘½éŒ¯èª¤\n",
        "        error_msg = f\"âŒ è‡´å‘½éŒ¯èª¤ç™¼ç”Ÿ: {e}\"\n",
        "        status_log += error_msg\n",
        "        raise gr.Error(f\"è‡´å‘½éŒ¯èª¤: {e}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. Gradio ä»‹é¢è¨­è¨ˆ\n",
        "# ==============================================================================\n",
        "\n",
        "# é…ç½® Gradio ä»‹é¢\n",
        "with gr.Blocks(title=\"å…¨è‡ªå‹•åŒ–æ–‡æœ¬æ´å¯Ÿç”Ÿæˆå™¨\") as demo:\n",
        "    gr.Markdown(\"# ğŸ¤– å…¨è‡ªå‹•æ–‡æœ¬æ´å¯Ÿç”Ÿæˆå™¨ (Gradio + Gemini + Google Sheets)\")\n",
        "    gr.Markdown(\n",
        "        \"æ­¤æ‡‰ç”¨ç¨‹å¼è‡ªå‹•åŸ·è¡Œ **çˆ¬èŸ² â†’ å¯«å…¥ Sheets â†’ è©é »åˆ†æ â†’ å›å¯« Sheets â†’ Gemini æ´å¯Ÿç”Ÿæˆ** çš„å®Œæ•´æµç¨‹ã€‚\\n\"\n",
        "        \"**ğŸš¨ é‡è¦ï¼šè«‹å‹™å¿…å°‡ JSON æ†‘è­‰æª”æ¡ˆå‘½åç‚º `service_account_key.json` ä¸¦ä¸Šå‚³ï¼**\"\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        # è¼¸å…¥å€\n",
        "        with gr.Column(scale=1):\n",
        "            target_url_input = gr.Textbox(\n",
        "                label=\"Step 1: çˆ¬èŸ²ç›®æ¨™ URL (è«‹è¼¸å…¥ä¸€å€‹å–®é æ–‡ç« é€£çµ)\",\n",
        "                value=\"https://www.ettoday.net/news/20241026/2847285.htm\",\n",
        "                placeholder=\"ä¾‹å¦‚: https://example.com/news/1\"\n",
        "            )\n",
        "            sheet_id_input = gr.Textbox(\n",
        "                label=\"Step 2: Google Sheet URL æˆ– ID (è«‹å‹™å¿…å¡«å¯«)\",\n",
        "                value=\"https://docs.google.com/spreadsheets/d/1cFttfMP93Bdzhtcqid06xTCqqZ0h5lrFODHXk8Se5R0/edit?gid=1806628729#gid=1806628729\",\n",
        "                placeholder=\"https://docs.google.com/spreadsheets/d/...\"\n",
        "            )\n",
        "            run_button = gr.Button(\"ğŸš€ ä¸€éµåŸ·è¡Œåˆ†ææµç¨‹\", variant=\"primary\")\n",
        "\n",
        "        # ç‹€æ…‹è¼¸å‡ºå€\n",
        "        with gr.Column(scale=1):\n",
        "            status_output = gr.Textbox(\n",
        "                label=\"æµç¨‹åŸ·è¡Œç‹€æ…‹ç´€éŒ„\",\n",
        "                lines=10,\n",
        "                interactive=False,\n",
        "                placeholder=\"ç­‰å¾…åŸ·è¡Œ...\"\n",
        "            )\n",
        "\n",
        "    # çµæœè¼¸å‡ºå€\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"ğŸ“Š é—œéµè©çµ±è¨ˆçµæœ (å›å¯«åˆ° Google Sheet)\"):\n",
        "            keywords_output = gr.Dataframe(\n",
        "                label=f\"å‰ {TOP_N_KEYWORDS} å€‹ç†±è©åŠå…¶ TF-IDF å¹³å‡å¾—åˆ† (å·²å›å¯«åˆ° Sheets)\",\n",
        "                headers=[\"è©å½™\", \"å¹³å‡TFIDFå¾—åˆ†\"],\n",
        "                datatype=[\"str\", \"str\"],\n",
        "                wrap=True\n",
        "            )\n",
        "        with gr.TabItem(\"ğŸ¤– Gemini æ´å¯Ÿæ‘˜è¦èˆ‡çµè«–\"):\n",
        "            gemini_insight_output = gr.Markdown(\n",
        "                label=\"AI ç”Ÿæˆçš„æ´å¯Ÿæ‘˜è¦èˆ‡ 120 å­—çµè«–\",\n",
        "                value=\"Gemini API è¼¸å‡ºçµæœå°‡é¡¯ç¤ºåœ¨é€™è£¡...\"\n",
        "            )\n",
        "\n",
        "    # ç¶å®šäº‹ä»¶\n",
        "    run_button.click(\n",
        "        fn=automated_analysis_flow,\n",
        "        inputs=[target_url_input, sheet_id_input],\n",
        "        outputs=[status_output, keywords_output, gemini_insight_output]\n",
        "    )\n",
        "\n",
        "# å•Ÿå‹• Gradio æœå‹™\n",
        "if __name__ == \"__main__\":\n",
        "    # è¼‰å…¥ jieba è©å…¸ï¼Œæº–å‚™åˆ†è©\n",
        "    jieba.initialize()\n",
        "    print(\"Jieba åˆ†è©åº«å·²åˆå§‹åŒ–ã€‚\")\n",
        "\n",
        "    demo.launch()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jieba åˆ†è©åº«å·²åˆå§‹åŒ–ã€‚\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a930accd6b6b8cd751.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a930accd6b6b8cd751.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "nKyRl2_xS3lc",
        "outputId": "245db946-6763-4367-a842-c20bfe4e1b97"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}